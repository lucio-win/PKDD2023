{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 15\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from dataset_utils import DataLoader\n",
    "from utils import random_planetoid_splits\n",
    "from GNN_models import GPRGNN_conv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(15)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg: Namespace(Gamma=None, Init='Random', K=10, RPMAX=10, alpha=0.1, auto=False, dataset='squirrel', dprate=0, dropout=0.5, early_stopping=200, epochs=1000, hidden=64, lr=0.002, ppnp='GPR_prop', train_rate=0.6, val_rate=0.2, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "import argparse\n",
    "from dataset_utils import DataLoader\n",
    "from utils import random_planetoid_splits\n",
    "from GNN_models import *\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from pytorch_lightning import seed_everything\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "parser.add_argument('--K', type=int, default=10)\n",
    "parser.add_argument('--alpha', type=float, default=0.1)\n",
    "parser.add_argument('--hidden', type=int, default=64)\n",
    "\n",
    "parser.add_argument('--dropout', type=float, default=0.5)\n",
    "parser.add_argument('--dprate', type=float, default=0)\n",
    "\n",
    "parser.add_argument('--Init', type=str,\n",
    "                    choices=['SGC', 'PPR', 'NPPR', 'Random', 'WS', 'Null'],\n",
    "                    default='Random')\n",
    "parser.add_argument('--Gamma', default=None)\n",
    "parser.add_argument('--ppnp', default='GPR_prop',\n",
    "                    choices=['PPNP', 'GPR_prop'])\n",
    "\n",
    "parser.add_argument('--dataset', default='squirrel', choices=[\n",
    "                    'chameleon', 'squirrel', 'film'])\n",
    "parser.add_argument('--train_rate', type=float, default=0.6)\n",
    "parser.add_argument('--val_rate', type=float, default=0.2)\n",
    "\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "parser.add_argument('--early_stopping', type=int, default=200)\n",
    "parser.add_argument('--lr', type=float, default=0.002)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0005)\n",
    "parser.add_argument('--RPMAX', type=int, default=10)\n",
    "\n",
    "parser.add_argument('--auto', default=False)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "print('arg:', args)\n",
    "\n",
    "seed_everything(15)\n",
    "dname = args.dataset\n",
    "dataset, data = DataLoader(dname)\n",
    "Init = args.Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = T.RandomLinkSplit(num_val=0.1, num_test=0.05, is_undirected=True,\n",
    "                      add_negative_train_samples=False)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_data, val_data, test_data = data\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([168720])\n",
      "torch.Size([39698])\n",
      "torch.Size([19848])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.edge_label.shape)\n",
    "print(val_data.edge_label.shape)\n",
    "print(test_data.edge_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = GPRGNN_conv(in_channels, out_channels, args=args)\n",
    "        self.edge_proj1 = torch.nn.Linear(out_channels*2, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.encoder(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "   \n",
    "    def decode_edge(self, z, edge_label_index):\n",
    "        z = torch.dropout(z, self.dropout, train=self.training)\n",
    "        h = torch.cat([z[edge_label_index[0]], z[edge_label_index[1]]], dim=1)\n",
    "        h = self.edge_proj1(h)\n",
    "        return h\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.3826, Val: 0.9312, Test: 0.9293\n",
      "[ 0.44827567 -0.05709306  0.1695483   0.00950561  0.09784034  0.00276275\n",
      " -0.03042223 -0.15381549 -0.21595468 -0.25329435 -0.04284211]\n",
      "Epoch: 200, Loss: 0.3724, Val: 0.9336, Test: 0.9320\n",
      "[ 0.39782367 -0.11272177  0.24843474  0.07444385  0.15271318  0.06123738\n",
      " -0.00939125 -0.14402719 -0.24750811 -0.30726757 -0.13649332]\n",
      "Epoch: 300, Loss: 0.3673, Val: 0.9356, Test: 0.9338\n",
      "[ 0.37302589 -0.15991681  0.28408788  0.12392276  0.17925603  0.11345807\n",
      "  0.00547589 -0.12364276 -0.26346194 -0.33278888 -0.19528616]\n",
      "Epoch: 400, Loss: 0.3679, Val: 0.9366, Test: 0.9348\n",
      "[ 0.36636784 -0.19421816  0.2957065   0.15777214  0.18692592  0.15211853\n",
      "  0.0108205  -0.10718151 -0.27748144 -0.35051948 -0.24023334]\n",
      "Epoch: 500, Loss: 0.3629, Val: 0.9371, Test: 0.9354\n",
      "[ 0.36955899 -0.21966779  0.29575787  0.18633027  0.18771939  0.18895886\n",
      "  0.01564491 -0.08736831 -0.2862714  -0.36003561 -0.27510558]\n",
      "Epoch: 600, Loss: 0.3616, Val: 0.9376, Test: 0.9360\n",
      "[ 0.37494104 -0.24005619  0.28472088  0.20627189  0.18182825  0.21877191\n",
      "  0.0176035  -0.06949064 -0.2928265  -0.36532317 -0.30191245]\n",
      "Epoch: 700, Loss: 0.3602, Val: 0.9377, Test: 0.9359\n",
      "[ 0.37789658 -0.25704431  0.26996823  0.22399784  0.17389005  0.25188777\n",
      "  0.02165326 -0.04684186 -0.29675706 -0.3678061  -0.32864445]\n",
      "Epoch: 800, Loss: 0.3595, Val: 0.9377, Test: 0.9361\n",
      "[ 0.37981226 -0.26444079  0.26340654  0.23163212  0.17099618  0.26893585\n",
      "  0.0255204  -0.03396835 -0.29653357 -0.36712077 -0.33988955]\n",
      "Epoch: 900, Loss: 0.3644, Val: 0.9381, Test: 0.9364\n",
      "[ 0.38934996 -0.27863778  0.24970596  0.23792896  0.16009549  0.29384474\n",
      "  0.02704644 -0.016031   -0.301808   -0.37183792 -0.36693007]\n",
      "Epoch: 1000, Loss: 0.3612, Val: 0.9384, Test: 0.9367\n",
      "[ 0.38024044 -0.28779989  0.23858452  0.24411099  0.15400647  0.31798328\n",
      "  0.03331747  0.004319   -0.29979875 -0.36904581 -0.38217538]\n",
      "Final Test: 0.9368\n"
     ]
    }
   ],
   "source": [
    "model = Net(dataset.num_features, dataset.num_classes, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=0)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# lamb = 0.1\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse').to(device)\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out_edge = model.decode_edge(z, edge_label_index).view(-1)\n",
    "    # out_class = model.decode_class(z)\n",
    "    loss_edge = criterion(out_edge, edge_label)\n",
    "    # loss_class = torch.nn.functional.nll_loss(out_class[train_data.train_mask], train_data.y[train_data.train_mask])\n",
    "    loss = loss_edge\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_edge(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode_edge(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    val_auc = test_edge(val_data)\n",
    "    test_auc = test_edge(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        best_temp = model.encoder.prop1.temp.detach().cpu().numpy()\n",
    "        best_model_dict = deepcopy(model.state_dict())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "        print(best_temp)\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    f'./results/gamma_{args.K}_{args.Init}_{args.dataset}_gprgnn_unsupervised.npy', best_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic(train_z, train_y, test_z, test_y, solver='lbfgs',\n",
    "             multi_class='auto', *args, **kwargs):\n",
    "        r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
    "        task.\"\"\"\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "        clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
    "                                 **kwargs).fit(train_z.detach().cpu().numpy(),\n",
    "                                               train_y.detach().cpu().numpy())\n",
    "        return clf.score(test_z.detach().cpu().numpy(),\n",
    "                         test_y.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label rate:  0.7998461834262642\n"
     ]
    }
   ],
   "source": [
    "# now we load the original dataset and split the dataset\n",
    "dname = args.dataset\n",
    "dataset, data = DataLoader(dname)\n",
    "data = data.to(device)\n",
    "train_rate = args.train_rate\n",
    "val_rate = args.val_rate\n",
    "percls_trn = int(round(train_rate*len(data.y)/dataset.num_classes))\n",
    "val_lb = int(round(val_rate*len(data.y)))\n",
    "TrueLBrate = (percls_trn*dataset.num_classes+val_lb)/len(data.y)\n",
    "print('True Label rate: ', TrueLBrate)\n",
    "permute_masks = random_planetoid_splits\n",
    "data = permute_masks(data, dataset.num_classes, percls_trn, val_lb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22478386167146974\n"
     ]
    }
   ],
   "source": [
    "# now we load the model with best weights\n",
    "model.load_state_dict(best_model_dict)\n",
    "model.eval()\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "acc = test_logistic(z[data.train_mask], data.y[data.train_mask],\n",
    "                     z[data.test_mask], data.y[data.test_mask], max_iter=150)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec73edf524274c519680832271d6059fda2d15ce575ac87c1bc205d8c723cc3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
