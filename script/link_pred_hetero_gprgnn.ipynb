{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataset_utils import DataLoader\n",
    "from utils import random_planetoid_splits\n",
    "from GNN_models import GPRGNN_conv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(15)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "import argparse\n",
    "from dataset_utils import DataLoader\n",
    "from utils import random_planetoid_splits\n",
    "from GNN_models import *\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from pytorch_lightning import seed_everything\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "# gpr参数\n",
    "parser.add_argument('--K', type=int, default=10)\n",
    "parser.add_argument('--alpha', type=float, default=0.1)\n",
    "parser.add_argument('--hidden', type=int, default=64)\n",
    "\n",
    "parser.add_argument('--dropout', type=float, default=0.5)\n",
    "parser.add_argument('--dprate', type=float, default=0)\n",
    "\n",
    "parser.add_argument('--Init', type=str,\n",
    "                    choices=['SGC', 'PPR', 'NPPR', 'Random', 'WS', 'Null'],\n",
    "                    default='Random')\n",
    "parser.add_argument('--Gamma', default=None)\n",
    "parser.add_argument('--ppnp', default='GPR_prop',\n",
    "                    choices=['PPNP', 'GPR_prop'])\n",
    "\n",
    "# 数据参数\n",
    "parser.add_argument('--dataset', default='chameleon', choices=[\n",
    "                    'chameleon', 'squirrel', 'film'])\n",
    "parser.add_argument('--train_rate', type=float, default=0.6)\n",
    "parser.add_argument('--val_rate', type=float, default=0.2)\n",
    "\n",
    "\n",
    "# 训练参数\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "parser.add_argument('--early_stopping', type=int, default=200)\n",
    "parser.add_argument('--lr', type=float, default=0.002)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0005)\n",
    "parser.add_argument('--RPMAX', type=int, default=10)\n",
    "\n",
    "# 模式选择\n",
    "parser.add_argument('--auto', default=False)\n",
    "\n",
    "# jupytor 自动判别\n",
    "jupytor = True\n",
    "if jupytor:\n",
    "    args = parser.parse_known_args(\n",
    "        args=['--auto', 'True'])[0]\n",
    "    print(args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "if args.auto:\n",
    "    arg_list = []\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "    for k, v in config[args.dataset].items():\n",
    "        arg_list.append(\"--\"+k)\n",
    "        arg_list.append(v)\n",
    "\n",
    "    arg_list.append('--dataset')\n",
    "    arg_list.append(args.dataset)\n",
    "\n",
    "    args = parser.parse_known_args(args=arg_list)[0]\n",
    "\n",
    "\n",
    "print('arg:', args)\n",
    "\n",
    "seed_everything(15)\n",
    "dname = args.dataset\n",
    "dataset, data = DataLoader(dname)\n",
    "Init = args.Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = T.RandomLinkSplit(num_val=0.1, num_test=0.05, is_undirected=True,\n",
    "                      add_negative_train_samples=False)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_data, val_data, test_data = data\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.edge_label.shape)\n",
    "print(val_data.edge_label.shape)\n",
    "print(test_data.edge_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = GPRGNN_conv(in_channels, out_channels, args=args)\n",
    "        self.edge_proj1 = torch.nn.Linear(out_channels*2, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.encoder(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "   \n",
    "    def decode_edge(self, z, edge_label_index):\n",
    "        z = torch.dropout(z, self.dropout, train=self.training)\n",
    "        h = torch.cat([z[edge_label_index[0]], z[edge_label_index[1]]], dim=1)\n",
    "        h = self.edge_proj1(h)\n",
    "        return h\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(dataset.num_features, dataset.num_classes, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=0)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# lamb = 0.1\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse').to(device)\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out_edge = model.decode_edge(z, edge_label_index).view(-1)\n",
    "    # out_class = model.decode_class(z)\n",
    "    loss_edge = criterion(out_edge, edge_label)\n",
    "    # loss_class = torch.nn.functional.nll_loss(out_class[train_data.train_mask], train_data.y[train_data.train_mask])\n",
    "    loss = loss_edge\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_edge(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode_edge(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    val_auc = test_edge(val_data)\n",
    "    test_auc = test_edge(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        best_temp = model.encoder.prop1.temp.detach().cpu().numpy()\n",
    "        best_model_dict = deepcopy(model.state_dict())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "        print(best_temp)\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    f'./results/gamma_{args.K}_{args.Init}_{args.dataset}_gprgnn_unsupervised.npy', best_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic(train_z, train_y, test_z, test_y, solver='lbfgs',\n",
    "             multi_class='auto', *args, **kwargs):\n",
    "        r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
    "        task.\"\"\"\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "        clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
    "                                 **kwargs).fit(train_z.detach().cpu().numpy(),\n",
    "                                               train_y.detach().cpu().numpy())\n",
    "        return clf.score(test_z.detach().cpu().numpy(),\n",
    "                         test_y.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we load the original dataset and split the dataset\n",
    "dname = args.dataset\n",
    "dataset, data = DataLoader(dname)\n",
    "data = data.to(device)\n",
    "train_rate = args.train_rate\n",
    "val_rate = args.val_rate\n",
    "percls_trn = int(round(train_rate*len(data.y)/dataset.num_classes))\n",
    "val_lb = int(round(val_rate*len(data.y)))\n",
    "TrueLBrate = (percls_trn*dataset.num_classes+val_lb)/len(data.y)\n",
    "print('True Label rate: ', TrueLBrate)\n",
    "permute_masks = random_planetoid_splits\n",
    "data = permute_masks(data, dataset.num_classes, percls_trn, val_lb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# now we load the model with best weights\n",
    "model.load_state_dict(best_model_dict)\n",
    "model.eval()\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "acc = test_logistic(z[data.train_mask], data.y[data.train_mask],\n",
    "                     z[data.test_mask], data.y[data.test_mask], max_iter=150)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adversarial-attacks-pytorch)",
   "language": "python",
   "name": "pycharm-c1f67ac9"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bb5364079b98e01f759fb95fd081907b02a8fbb4e4b2accc4da2526c8105a6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
